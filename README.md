### 230804 总述

RL 估计器 主要参考22.Linear Observer Learning by Temporal Difference一文，把该文献中在线性系统上用 RL 学习协方差矩阵 P 的想法拓展到非线性系统。

系统动态来自于22.Moving horizon estimation for nonlinear and non-Gaussian stochastic disturbances一文。
$$
\begin{align}
&x_{1,k+1} = 0.99x_{1,k} + 0.2x_{2,k}
\\
&x_{2,k+1} = -0.1x_{1,k} + \frac{0.5x_{2,k}}{1+x_{2,k}^2} + w_k
\\
&y_k \quad\ \ = x_{1,k} - 3x_{2,k} + v_k
\\
&w_k \sim \mathcal{N}(0,1), v_k \sim \mathcal{N}(0,0.01)
\end{align}
$$
首先，明确我们的问题，是估计arrival cost。假定当前时刻为 k 时刻，时间窗口长度为 q ，那么MHE问题做的是已知窗口初始状态近似服从分布 $x_{k-q} \sim \hat{P}(x_{k-q})$ 和窗口观测序列 $ \{y_{k-q+1},y_{k-q+2}\dots,y_{k}\} $ 的条件下，给出当前时刻的估计 $\hat{x}_{k}$ ，而arrival cost 的估计就是基于初始时刻分布 $x_0 \sim P(x_0)$ 和过去时刻的观测序列 $\{y_1, y_2, \dots, y_{k-q}\}$ ，估计窗口初始状态PDF $\hat{P}(x_{k-q})$ 。

下一时刻 k+1 时刻，窗口往后移动，MHE问题变为已知 $\hat{P}_{k-q+1}$ （ $\hat{P}(x_{k-q+1})$ 的简写）和观测序列 $\{y_{k-q+2},\dots,y_{k+1}\}$ ，估计 $\hat{x}_{k+1}$ ，arrival cost 做的仍然是基于初始分布 $P_0$ 和过去的观测序列 $\{y_1,\dots,y_{k-q+1}\}$ 得到 $\hat{P}_{k-q+1}$ 。

可以看到，MHE的主体优化问题部分，结构维度形式都没有发生变化。而arrival cost 的估计则是增加了一维。但是在实际计算过程中，由于过去的观测序列维度较高难以处理，因此一般对arrival cost 的近似是以递归的方式进行（可以用MHE估计arrival cost吗，会有啥提升吗？），即假定前面的估计是最优的情况下，只做当前的单步估计，基于 $\hat{P}_{k-q}$ 和观测 $y_{k-q+1}$ 估计 $\hat{P}_{k-q+1}$ 。

以上都有概率意义下的详细解释，具体可以参考22.Moving horizon estimation for nonlinear and non-Gaussian stochastic disturbances一文。

那么我们所希望的是能够更快更好地得到窗口初始状态的估计（即arrival cost的估计），并且为了方便后续MHE优化问题的求解，限制其为高斯分布（对应的arrival cost为二次型）。当然如果有别的形式能更好拟合非高斯分布，并且对应的arrival cost形式也能方便呢优化问题的求解，那就更好了。也就是说，在 k 到 k+1 步，强化学习智能体需要基于 $\{\hat{x}_{k-q},\hat{P}_{k-q},y_{k-q+1}\}$ （即，上一时刻分布的估计，由于限定形式为高斯分布，所以对应到均值和协方差，以及这一时刻的观测），给出初始状态分布的估计（均值由非线性系统动态给出，实际上只需要给出协方差矩阵 $\hat{P}_{k-q+1}$ ）。这其实就是一个单步估计的问题，所以我们可以先把它剥离出MHE的体系，只是基于 $\{\hat{x}_{k},\hat{P}_{k},y_{k+1}\}$ 来估计 $\hat{P}_{k+1}$ 。

后续是代码说明文档，在代码文件中，双井号（##）索引的注释一般是未解决的疑问，需要不时进行回顾。



### 20230804 系统动态

dynamic.py

step函数：{

功能：系统动态方程，由当前时刻状态得到下一时刻状态

输入：x—2维k时刻状态量(ndarray, (2, ))，disturb—2维过程扰动项(ndarray, (2, ))，noise—1维测量噪声(float64)

输出：x_next—2维k+1时刻状态量(ndarray, (2, ))，y_next—1维k+1时刻观测量(float64)

备注：使用时需注意，相同的随机数种子会生成相同的噪声，因此在不同的时间步需要不同的随机数种子输入。并且在不同次的仿真过程中，随机数种子的顺序应该不同。由于需要保证可复现性，因此初始状态和噪声序列对于指定的一次仿真过程应该是确定的，即指定仿真序号 i 后，初始状态和噪声序列就确定了。那么在仿真执行过程中，从头到尾需要保存一个噪声序列。

}

reset函数：{

功能：为第 i 次仿真过程生成初始状态和噪声序列

输入：sim_num—仿真序列号(int)，maxstep—仿真最大时间步长(int)，#x0_mu—初始状态均值(ndarray, (2, ))，#P0—初始状态协方差矩阵(ndarray, (2,2))，#disturb_mu—扰动项均值(ndarray, (2, ))，#disturb_Q—扰动项协方差矩阵(ndarray, (2,2))，#noise_mu—噪声项均值(float)，#noise_R—噪声项协方差(float)

输出：initial_state—初始状态(ndarray, (2, ))，disturb_list—扰动序列(ndarray, (200,2))，noise_list—噪声序列(ndarray, (200, ))

备注：#为可选参数。目前扰动和噪声都是一维的，所以PQ都是浮点数，而不是协方差矩阵，后续如果有需要再做修改。disturb_list是从w0开始的，而noise_list是从v1开始的，即disturb_list[0]=w0，noise_list[0]=v1。disturb_mu和disturb_Q的默认值偷懒没写成ndarray类型而是直接用的list型，但是传进去ndarray型也能正常运行。

}



### 20230805 EKF和simulation

estimator.py

inv函数：{

功能：矩阵求逆

输入：M—矩阵(ndarray, (n,n))

输出：M矩阵的逆(ndarray, (n,n))

备注：由于numpy提供的矩阵求逆的方法不能对1维矩阵求逆，并且有两个前缀写起来不方便，因此简单整合了一下。矩阵M的维度一定要是n*n，即使n=1，因为里面的判断条件是M.shape==(1,1)，而且一维矩阵求逆肯定是为了矩阵运算的，在矩阵运算中也会要求矩阵有两个维度，而不是只有一个维度。

}

EKF函数：{

功能：EKF做单步估计

输入：state—当前状态（的估计）(ndarray, (2, ))，P—当前状态的协方差矩阵（的估计）(ndarray, (2,2))，obs_next—下一时刻的观测(float64)，#Q—过程噪声协方差矩阵(ndarray, (2,2))，#R—测量噪声方差(ndarray, (1,1))

输出：state_hat—下一时刻状态的估计(ndarray, (2, ))，P_hat—下一时刻状态的协方差矩阵的估计(ndarray, (2,2))

备注：预测和更新都是直接照搬的12.A Fresh Look at the Kalman Filter P12最下方的公式。做出来的效果与22.Moving horizon estimator for nonlinear and non-Gaussian stochastic disturbances中P243 case Ia接近，初步认为写的EKF没什么大问题。

}

RL_estimator.py

simulate函数：{

功能：进行一次状态估计的仿真过程

输入：args—一些全局的参数，sim_num—仿真序号（对应该次仿真的随机数种子）(int)，STATUS—指明用哪种估计器（EKF、NLS-EKF、NLS-RLF或者train）(str)

输出：x_pre_seq—状态预测序列(list)，y_seq—观测序列(list)，P_seq—预测协方差序列(list)

备注：当STATUS!='train'时，会绘制状态估计的图以及误差的图。x_pre_seq和P_seq都是从1时刻开始存的，而y_seq是从0时刻开始存的，因此在传出参数后，需要人为在x_pre_seq最前面加上x0_mu作为0时刻的预测。

}



### 20230808 非线性最小二乘估计器

estimator.py

block_diag函数：{

功能：将多个矩阵拼成一个块对角矩阵

输入：matrix_list—矩阵列表(list)

输出：bd_M—块对角矩阵(ndarray)

备注：矩阵列表中每一个元素都是一个ndarray，对其维度没有限制。块对角矩阵是按照从左上到右下来排列列表中的矩阵，实际使用时，可以直接将多个矩阵用方括号括起来作为输入，例如[A, B, C]。

}

NLSF函数：{

功能：非线性最小二乘求解单步优化问题得到状态估计

输入：state_mu—k时刻状态估计(ndarray, (2, ))，P—k时刻状态协方差估计(ndarray, (2,2))，obs_next—k+1时刻观测(float64)，Q—（k+1时刻）过程噪声协方差矩阵(ndarray, (2,2))，R—（k+1）时刻测量噪声协方差矩阵(ndarray, (2,2))

输出：result.x—状态估计值(ndarray, (4, ))

备注：输出的是k时刻状态估计的修正值（前2维）以及k+1时刻状态的估计值（后2维）。基于EKF估计P矩阵的NLS-EKF方法效果比单纯EKF好一点，目前初步认为NLSF函数写的没问题。

}

residual_fun函数：{

功能：非线性最小二乘求解器中需要的残差向量

输入：x—待优化变量(ndarray, (4, ))，state_mu、P、obs_next、Q、R—NLSF函数的输入直接传过来的。

输出：f—残差(ndarray, (5, ))

备注：残差函数实际上是
$$
[\hat{x}_k - \bar{x}_k, \hat{x}_{k+1} - f(\hat{x}_k), y_{k+1} - h(\hat{x}_{k+1})]
diag(P_k^{-1}, Q_{k+1}^{-1}, R_{k+1}^{-1})
\triangle ^\top
$$
所以残差向量是前面这一串向量乘上中间的对角矩阵的cholesky分解。

}

jac_fun函数：{

功能：非线性最小二乘求解器中需要的雅可比矩阵

输入：x—待优化变量(ndarray, (4, ))，state_mu、P、obs_next、Q、R—NLSF函数的输入直接传过来的。

输出：J—雅可比矩阵(ndarray, (5,4))

备注：第一行是残差向量第一个分量关于四个决策变量求偏导，后续同理。

}

同时对之前的内容进行了一些修订。另外，由于RL部分好像与常规DDPG算法不太相同，在本例中Critic函数好像是有解析的表达形式，而不是像一般的算法中需要用另一个神经网络去拟合，因此关于RL部分的代码暂时不在本文档中进行说明。

在class中，如果在init方法中指定随机数种子，那么反复进入该类的其余方法时，会生成不同的随机数，但这组随机数是可复现的，也就是对应于init方法中指定的随机数种子。因此可以把dynamic.step函数改成class中的一个方法，可能是更简单的写法。



### 20230812 代码修正

对之前的代码中的部分错误进行修正：

- estimator.py中对R的初始值设定都是1，args中对R的默认值也是1，然而实际上应该是0.01。

但是修正之后，NLS-EKF突然效果变得很差，检查了一下jac矩阵应该也是没问题的，不过实际上把非线性最小二乘中的jac参数删去之后，效果就跟原来一样好了，这一点让我很疑惑。



### 20230816 代码修正、存档

之前的代码，都是按照 $(x_{k|k}, y_{k+1}) \rightarrow x_{k+1|k+1}$ 来写的，之前也提到过，用非线性最小二乘求解器（least_squares）求解时，jac矩阵会产生错误的影响，并且即使去掉jac矩阵，或是换用优化问题的求解器（minimize），训练得到的结果也非常差。

现在按照论文中的形式，改写成了 $(x_{k|k-1}, y_{k}) \rightarrow (x_{k|k}, x_{k+1|k})$ ，也就是预测值之间的递推，协方差矩阵P的估计，仍然是上一时刻估计当前时刻，即 $(x_{k|k-1}, y_{k}) \rightarrow P_{k+1|k}$ ，其实是很直观的，输入信息是确定好的，而考虑到 $y_k$ 不会对 $k$ 时刻的先验产生影响，它只会对当前时刻的修正产生影响，进而影响到后一时刻的先验，即 $P_{k+1|k}$ 。

改写后的代码在文件名后面都加了_copy，以于之前的代码区分开。在改写后，目前是得到了与EKF接近的训练参数，并且在不同随机数种子的仿真中效果都比较接近，因此先对此版本进行存档。并且附上效果图：(还要补充仅初始化、未训练的图)

图1：EKF在随机数种子10086、123456下的仿真效果

![](https://z4a.net/images/2023/08/16/EKF10086.png)

![](https://z4a.net/images/2023/08/16/EKF123456.png)

图2：NLS-EKF在随机数种子10086、123456下的仿真效果

![](https://z4a.net/images/2023/08/17/NLS-EKF10086.png)

![](https://z4a.net/images/2023/08/17/NLS-EKF123456.png)

图3：NLS-RLF在随机数种子10086、123456下的仿真效果（没有NLS-EKF效果好）

![](https://z4a.net/images/2023/08/16/RLF10086.png)

![](https://z4a.net/images/2023/08/16/RLF123456.png)

图4：仅做初始化，未做后续训练时，NLS-RLF在10086、123456下效果图

![](https://z4a.net/images/2023/08/17/RLF010086.png)

![](https://z4a.net/images/2023/08/17/RLF0123456.png)



### 20230816 训练部分代码

RL_estimator_copy.py

OUnoise类：{

功能：产生OU过程噪声，具体可以参考https://blog.csdn.net/qq_33254870/article/details/105137275。OU过程是时序相关的，所以在强化学习的前一步和后一步的动作选取过程中可以利用OU过程产生时序相关的探索，以提高在惯性系统（即环境）中的控制任务的探索效率。但是由于目前的探索确实应该用时序不相关的噪声，因此OU噪声现在没用上，细节就也先不写了。

}

Actor类：{

功能：神经网络，输入为状态和观测，输出为P逆和h。状态和观测叠起来，经过全连接到400神经元的隐藏层1，然后经过relu激活函数，再过一个全连接到300神经元的隐藏层2，再进过relu激活函数，最后经过全连接到输出。

init方法：{

输入：state_dim—状态量维度(int)，obs_dim—观测量维度(int)，#h1—隐藏层1的神经元数量(int, 400)，#h1—隐藏层2的神经元数量(int, 300)}

forward方法：{

输入：state_pre—当前时刻状态量的先验估计 $x_{k|k-1}$ (ndarray, (2, ))，obs—当前时刻的观测值 $y_k$ (ndarray, (1, ))

输出：action—向量形式的输出，后续将其转换为P逆和h(tensor, (4, )}

update_weight方法：{

输入：bsp—批状态先验(list)，bo—批观测(list)，bPni—批下一时刻协方差矩阵的逆(list)，bh—批下一时刻h(list)，#lr—神经网络更新的学习率，具体起到什么作用还不太清楚(float, 1e-3)

输出：无。

备注：如果Pni有非正定的，会直接跳过不更新。}

}

ReplayBuffer类：{

功能：存储经验回放的数据。

init方法：{

输入：maxsize—最大存储数量（不算初始经验样本）(int)}

push_init方法：{

功能：加入初始经验样本，在训练过程中这些样本将不会被替换或抹去。

输入：state_pre—状态先验估计(ndarray, (2, ))，obs—观测值(ndarray, (1, ))，Pinv_next—下一时刻状态先验估计对应的协方差矩阵(ndarray, (2,2))，h_next—下一时刻价值函数中的常数项(float)}

push方法：{

功能：加入经验样本，当样本数量达到最大存储数量后，新样本会替换最老的样本

输入：state_pre—状态先验估计(ndarray, (2, ))，obs—观测值(ndarray, (1, ))，Pinv_next—下一时刻状态先验估计对应的协方差矩阵(ndarray, (2,2))，h_next—下一时刻价值函数中的常数项(float)}

sample方法：{

功能：采样指定数量的样本

输入：n—采样数量(int)

输出：bsp，bo，bPi，bh—批样本，均为list类型，list中每个元素对应类型为样本输入的类型}

}

RL_estimator类：{

功能：整合actor和critic，相当于一个agent

init方法：{

输入：state_dim—状态量维度(int)，obs_dim—观测量维度(int)，noise—探索噪声项(class)，#rand_num—随机数种子号(int, 111)，#STATUS—选择训练（'train'）或是测试(str, 'train')

备注：noise目前没有启用，因为感觉更像是训练神经网络，而不是RL中的探索，因此STATUS就保持输入为'test'。但rand_num是给神经网络的随机初始化指定种子号，因此这个不能缺少，不过这个在一次训练中只需要做一次初始化就是了。}

value方法：{

功能：计算价值函数的值。

输入：state—状态(ndarray, (2, ))，state_pre—状态的先验估计(ndarray, (2, ))，Pinv—状态先验估计的协方差矩阵的逆(ndarray, (2,2))，h—常数项(float)

输出：Q—价值函数的值(float)

备注：就是平方式加上常数项，其实别的平方式也可以用这个函数算，但是为了区别开价值函数，就没有用来算别的平方式。}

get_Pinv方法：{

功能：输入状态先验和观测，得到下一时刻先验协方差矩阵的逆和h

输入：state_pre—状态的先验估计(ndarray, (2, ))，obs—观测值(ndarray, (1, ))

输出：P_next_inv—下一时刻先验协方差矩阵的逆(ndarray, (2,2))，h_next—下一时刻价值函数常数项(float)

备注：动作网络输出action的是P的cholesky分解下三角L矩阵的下三角元素，因此首先将action排列成下三角的矩阵，然后Pinv=L@L.T，另外action最后一项直接就是h。}

reset_noise方法：{

备注：之前想用这个方法来重设噪声中的随机数种子，但目前噪声没用上，因此这个方法也没用。}

}

train函数：{

功能：训练策略网络

输入：args—参数空间，agent—RL_estimator类(class)，replay_buffer—经验回放池(class)

流程：以 i 表示训练集数（episode），t 表示每一集中的时间步。在每集开始时，首先进行初始状态 $x_{0}$ 和噪声序列 $w_{0:199}，v_{0:199}$ 。值得一提的是， $w$ 序列是 $w_0$ 开始，而 $v$ 序列是 $v_1$ 开始。因此另外生成随机噪声 $v_0$ 以得到初始观测 $y_0$ 。然后赋值初始状态的先验估计x0_mu，P0。（这里有个疑问，因为我生成初始状态是从x0_mu和P0生成的，也就是相当于这俩不算是先验分布，而是真实分布，但我这里计算的时候把它当做先验分布在用，是否有问题？）接下来做时间步的循环，首先比较独立的一块是真实状态和观测的生成。然后是基于当前状态的先验和当前的观测，通过策略网络得到下一时刻的预测协方差矩阵。接下来是估计器部分，通过当前时刻的状态预测、先验协方差矩阵以及当前时刻的观测，求解非线性最小二乘问题，得到当前时刻的状态估计、下一时刻的状态预测。然后将相关数据存储到经验样本池中，以便后续反复训练（不知道反复训练有没有道理，毕竟也相当于是异策略）。最后是计算平均平方误差MSE，以及新老数据交替。在做完一集的训练后，打印该集的MSE。如果MSE的两个分量都比记录的最低MSE还低，那么保存当前的模型，并且记录新低的MSE。

}

main函数：{

功能：将一些全局参数统一放在这里，以便修改。用EKF对网络做初始化训练，并作为初始经验样本保存。

}



### 20230818 修补及改进

- 尝试不采用EKF收集初始化样本，一方面得到的效果并不好，另一方面在训练过程中会出现很多随机梯度下降后的P矩阵非正定的情况。
- 只收集初始化样本，不直接进行初始化，得到的效果稍差于NLS-RLF，但虽然有warmup_step（200步），有初始化样本之后做不做初始化好像区别不大？所以这个结果好像也意义不大。



### 20230822 批训练改进

将原来的手动循环做批训练改成了自动的批训练，训练时间好像减少了许多，现在4层网络*300神经元，500集几分钟就训练好了。



### 20230831 参数调整

对一些训练参数以及网络隐藏层数进行了调整，在第一个仿真场景下得到了初步的效果。

![](picture/dynamic1.png)

训练参数以及仿真设置：

<img src="picture/paras_model10000train.png" style="zoom:80%;" />



### 20230905 进一步测试

- 对网络结构进行了修改，发现还是上次的10*200效果最好。
- 训练时是max_sim_steps=200的仿真长度，尝试在max_sim_steps=1000的仿真长度下进行测试，得到的效果如下，初步结论是训练所得到的网络能够学习出正确的依赖关系

![](picture/dynamic1(200trn1000sim).png)

- 去掉NLS，单用RLF？——在EKF和NLS-EKF之间，毕竟也不是按这种方式来训练的。

![](picture/dynamic1(RLFwithoutNLS).png)

- 增加窗口长度？
- 增加模型失配？——效果并不算好，原因可能在于训练过程本身显式依赖于模型，因为训练用的数据（TDerror）来自于依赖于模型的计算。

![](picture/dynamic1(model_mismatch).png)



### 20230915 错误但有效的结果

在输入中补充了h(以前只有P)，确实得到了进一步的提升，仿真效果如下，（对比之前的数据）效果略优于NLS-UKF，几乎与UKF持平

![](picture/dynamic1(P h input).png)